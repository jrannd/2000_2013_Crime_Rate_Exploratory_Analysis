# -*- coding: utf-8 -*-
"""Crime Rate in California 2000-2013.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17nAP03rKfZcZjYExLYxZmW4M7B8TsEsS

#Crime Rate in California 2000-2013 per 1,000

#Load data
"""

import requests
import pandas as pd
import io

url = 'https://data.chhs.ca.gov/dataset/99bc1fea-c55c-4377-bad8-f00832fd195d/resource/bc09f211-200c-4c4c-aa13-d2e89c0d5577/download/hci_crime_752_pl_co_re_ca_2000-2013_21oct15-ada.xlsx'

response = requests.get(url)

response.raise_for_status()
df = pd.read_excel(io.BytesIO(response.content))

# create back up of original df before we work on it

df_original = df.copy()

df.columns

df.info()

df.describe()

"""# Cleaning Data"""

# view unique values in ind_id column
df['ind_id'].unique()

df[df['ind_id'] == 'END OF TABLE']

"""This is the last row of the dataset and has no values in any column. Drop the above row, it is not needed."""

drop_index = df.index[df['ind_id']=='END OF TABLE'].to_list()

# shows the index of the row that we want to drop
print(drop_index)

#drops the row based on the index
df.drop(index=drop_index, inplace = True)

#check that last row does not show end of table
df[-1:]

#see if there is a reason to keep column ind_definition and ind_ind
x = df['ind_definition'].unique()
y = df['ind_id'].unique()
print(x,y)

"""Drop above columns, we know this is a data set that shows Number of Violent Crimes per 
1,000 Population. ind_id is not any sort of index and does not have a purpose for exploring this dataset."""

#including version number in this as well since it is in the df_original if we ever need to refernece.
df.drop(columns=['ind_definition', 'ind_id', 'version'], inplace = True)
df.head()

"""Explore why we have missing values in county and region columns"""

# isolate the location columns
location_df = df.iloc[:,3:15]
location_df.head()

# view where fips, county name, region code, and region name are null.

location_df[(location_df['county_fips'].isnull()) & (location_df['county_name'].isnull()) & (location_df['region_code'].isnull()) & (location_df['region_name'].isnull())]

"""Above we can see that the only time there is no location data in the columns county_fips, county name, region code, and region_name 
is when the geoname is California. This is showing that there is state level aggregations of each crime per year. 
This also explaiins why there are 70 null values in region_name. I will look to see if there are aggregations at the lower level like regional aggregations."""

# see if any data is returned when county fips is NOT null, while county_name, region_code and region_name ARE null
location_df[(location_df['county_fips'].notnull()) & (location_df['county_name'].isnull()) & (location_df['region_code'].isnull()) & (location_df['region_name'].isnull())]

"""When county_fips is not null and the remainder of columns are null, we yield no results meaning when 
county_fips has values we will see values in count_name, region_code, and region_name. Lets test this below. 
We will adjust one column at a time to include notnull() until all columns have notnull()"""

# when we try to see the location df where county_fips and county_name don't have null values but region_code and region_name have null values, we get no output.

location_df[(location_df['county_fips'].notnull()) & (location_df['county_name'].notnull()) & (location_df['region_code'].isnull()) & (location_df['region_name'].isnull())]

# when we try to see the location df where county_fips, county_name, and region_code don't have null values but region_name has null values, we get no output.

location_df[(location_df['county_fips'].notnull()) & (location_df['county_name'].notnull()) & (location_df['region_code'].notnull()) & (location_df['region_name'].isnull())]

# When we look at a the location df where all columns don't have null values, we do get a df output.

location_df[(location_df['county_fips'].notnull()) & (location_df['county_name'].notnull()) & (location_df['region_code'].notnull()) & (location_df['region_name'].notnull())]

"""When our county_fips column contains no null values, no  California aggregate numbers are present and we have complete data in the 
county_name, region_code, and region_name columns


""Why are there nulls in the fips and county name? Let us explore
"""

# view df where county_fips and county_name contain null values

location_df[(location_df['county_fips']).isnull() & (location_df['county_name'].isnull())]

# figure out which geoname and regions are present when county fips data is missing

x = location_df['geoname'][(location_df['county_fips']).isnull() & (location_df['county_name'].isnull())].unique()


y = location_df['region_name'][(location_df['county_fips']).isnull() & (location_df['county_name'].isnull())].unique()

print(f'{x}\n{y}')

# view an example of a region where county data is missing but region data is present.

location_df[(df['county_fips']).isnull() & (location_df['county_name'].isnull()) & (location_df['geoname'] == 'Butte')]

# pull suspected aggregate data at regional level from the above table
df.loc[3326]

# check that the above matches the the first column in the below where we pull data based on county fips number.

df[(df['county_fips'] == 06007.0)].head(1)

"""After the above steps we see that when the county_fips, county_name are null it is because we are looking at an aggregate view of crimes at the regional level, 
(geotype = 'RE'). Butte is one of the few regions in this table where the only county is Butte. At the regional level, Butte has a numerator of 475 for aggrevated assult in 2000
and when we call the county specific data, we can see that the numerator for aggrevated assult matches 475 in year 2000. 
Thus proving we have aggregations at state, regional, and county level. We can also reasonably assume that the nulls in the location specific columns 
are harmless and do not need to be removed.

Rename one of the reported crimes into something less graphic for viewers
"""

# view the different violent crimes
df['strata_level_name'].unique()

#view the data types for each column
df.dtypes

#convert data dype to string and check data type.
df['strata_level_name'] = df['strata_level_name'].convert_dtypes(convert_string = True)
df['strata_level_name'].dtype

# replace old crime name with new one
df['strata_level_name'] = df['strata_level_name'].replace('Forcible rape', 'Sexual assult')

# double check that the violent crime name changed.
df.iloc[0:5,0:18]

"""While changing the value of one of the crimes I noticed that there was a value named: Jurisdiction does not report. 
Lets dive deeper into this and see if we can drop these values"""

df['strata_level_name'].unique()

# create subset where jurisdiction does not report
df_dnr = df[df['strata_level_name'] == 'Jurisdiction does not report']

# see overview of the dataframe where jurisdiction does not report
df_dnr.info()

# all the crime data is null for these rows because these are all unincorporated communities. I do not see where these rows would have much importance to analyzing crime in california over 13 years. We will drop the rows

# take index of all rows in the unincorporated community data frame
dnr_indexes = df_dnr.index
print(dnr_indexes)

# drop unincorporated indexes from our data set
df.drop(index=dnr_indexes, inplace = True)
df.info()

"""We have dropped the unincorporated coummunity rows with no crime data. Almost done cleaning!"""

# explore the null values we have in the nummerator. Are they missing values/not reported or are they 0s
df_no_num = df[df['numerator'].isnull()]
df_no_num.head()

df[df['county_fips'] == 6003.0]

"""The nulls in numerator appear to be 0s not a lack of reporting, as the sum of crimes in a given year mathces the "violent crime total". double check this"""

pd.set_option('display.max_columns', None)

# change type for numerical values to integers

df['reportyear'] = df['reportyear'].convert_dtypes(convert_integer= True)
df['numerator'] = df['numerator'].convert_dtypes(convert_integer= True)
df['denominator'] = df['denominator'].convert_dtypes(convert_integer= True)

# take the violent crime total value and put it in a variable
x = df[df['strata_level_name_code'] == 5.0]['numerator']

# take the other 4 violent crime values and store them in a series
y = df[df['strata_level_name_code'] != 5.0]['numerator']

#replace nulls with 0s
y.fillna(0,inplace=True)

# in theory, the sum of both should be equal to each other.
print(x.sum())
print(y.sum())

# our sums don't add up and below is an example of that. There may have been some miscalculation or misreporting.

df[(df['geotype'] == 'CA') & (df['reportyear'] == 2000)]

# example of how above numbers do not match 'Violent crime total' in row 5
138325+9784+2079+60237

"""Assuming that null values are 0s and not miss reproted values"""

df['numerator'].fillna(0, inplace = True)

"""Since our violent crime totals seem to be off, we need to correctly add up the numerator colum so that the violent crime total is equal 
to all the recorded crimes that come before it."""

# Create a copy of df to work on
df_fixed = df.copy()

# sort df values
df_fixed.sort_values(['reportyear', 'geotype','geoname',  'strata_level_name_code'])

def calculate_corrected_total(group):
    # Create a mask for individual crime categories (codes 1-4)
    # creates a new bool column for values 1-4
    individual_crimes_mask = group['strata_level_name_code'].isin([1, 2, 3, 4])


    # Create a copy of the group
    group_copy = group.copy()

    # Calculate cumulative sum only for individual crimes
    group_copy['temp_cumsum'] = 0
    group_copy.loc[individual_crimes_mask, 'temp_cumsum'] = group_copy.loc[individual_crimes_mask, 'numerator'].cumsum()

    # For the total row (code 5), use the maximum cumsum value from individual crimes
    # create another bool column but for only if code is 5
    total_mask = group_copy['strata_level_name_code'] == 5
    if total_mask.any():
        correct_total = group_copy.loc[individual_crimes_mask, 'temp_cumsum'].max()
        group_copy.loc[total_mask, 'numerator'] = correct_total

    # Clean up temporary column
    group_copy = group_copy.drop('temp_cumsum', axis=1)
    return group_copy

# Apply the correction to each geographic group
grouping_cols = ['reportyear', 'geoname', 'geotype']
df_fixed = df_fixed.groupby(grouping_cols, group_keys=False).apply(calculate_corrected_total)

df_fixed.head(10)

df.head(10)

# take the violent crime total value and put it in a variable
x = df_fixed[(df_fixed['strata_level_name_code'] == 5.0) & (df_fixed['geotype'] == 'PL')]['numerator']

# take the other 4 violent crime values and store them in a series
y = df_fixed[(df_fixed['strata_level_name_code'] != 5) & (df_fixed['geotype'] == 'PL')]['numerator']



# in theory, the sum of both should be equal to each other.
print(x.sum())
print(y.sum())

# need to figure out why PL is off
df_fixed_PL = df_fixed[df_fixed['geotype'] == 'PL']

# we have 14 years worth of data for each city should appear 70 times in our data set. 1 for each violent crime plus once for the violent crime total

((df_fixed_PL['geoname'].value_counts().sort_index()) == 70).unique()

# count how many times each city is in our data
df_fixed_PL['geoname'].value_counts()

# take a look at the city with 140 instances
df_fixed_PL[(df_fixed_PL['geoname']== 'Porterville city') &  (df_fixed_PL['reportyear'] == 2000)]

# we are seeing that some of these cities are reporting twice for each crime. We need to drop duplicates
df_fixed1 = df_fixed.copy()

#drop rows based on the below columns. anything that has the same value in each one of these columns will be dropped
df_fixed1.drop_duplicates(subset=[ 'reportyear', 'geotype', 'geoname', 'county_fips', 'strata_level_name_code','numerator'],inplace = True)

# check
df_fixed1[df_fixed1['geotype'] == 'PL']['geoname'].value_counts()

# Clean Angels City

df_fixed1[df_fixed1['geoname' ]== 'Angels City city']['reportyear'].value_counts()

df_fixed1[df_fixed1['geoname' ]== 'Angels city']['reportyear'].value_counts()

# it is safe to assume that these are the same since Angel City city stops at 2009 and angel city starts at 2010. We will change Angels city to Angels City city.

df_fixed1['geoname'] = df_fixed1['geoname'].convert_dtypes(convert_string = True)

df_fixed1['geoname'] = df_fixed1['geoname'].replace('Angels city', 'Angels City city')

#change so geotype is the same too
df_fixed1['geotypevalue'] = df_fixed1['geotypevalue'].replace(2112.0, 2132.0)

# recalculate totals since we have deleted duplicate rows
df_fixed2 = df_fixed1.copy()

df_fixed2.sort_values(['reportyear', 'geotype','geoname',  'strata_level_name_code'])

df_fixed2 = df_fixed2.groupby(grouping_cols, group_keys=False).apply(calculate_corrected_total)

# No matter what the geotype is, CA, RE, CO, or PL we get the same amount when we compare the sum of all violent crimes to the sum of the total of violent crimes. Now Our data flows consitently.

# take the violent crime total value and put it in a variable
x = df_fixed2[(df_fixed2['strata_level_name_code'] == 5.0) & (df_fixed2['geotype'] == 'PL')]['numerator']

# take the other 4 violent crime values and store them in a series
y = df_fixed2[(df_fixed2['strata_level_name_code'] != 5) & (df_fixed2['geotype'] == 'PL')]['numerator']



# in theory, the sum of both should be equal to each other.
print(x.sum())
print(y.sum())

# there were null values we need to address in the denominator column (the population). About 2000.

df_fixed2.info()

df_fixed2[(df_fixed2['denominator'].isnull()) & df_fixed2['dof_population'].notnull()]['geoname'].value_counts()

# view example of region where denominator is null but we have dof_population data.
df_fixed2[df_fixed2['geoname']=='Amador City city']

# fill null values in denominator column with population data in dof population column

df_fixed2['denominator'] = df_fixed2['denominator'].fillna(df_fixed2['dof_population'])

# view places where we can not see any population data.

df_fixed2[(df_fixed2['denominator'].isnull()) & df_fixed2['dof_population'].isnull()]['geoname'].value_counts()

# now we must address the locations with no population at data all.
# lets see if any of them have reported crimes (values in numerator column)

df_fixed2[(df_fixed2['denominator'].isnull()) & df_fixed2['dof_population'].isnull()]['numerator'].unique()

# all these have no numerator/no reported crime, so we can drop these

df_fixed2[(df_fixed2['denominator'].isnull()) & df_fixed2['dof_population'].isnull()]

x = df_fixed2[(df_fixed2['denominator'].isnull()) & df_fixed2['dof_population'].isnull()].index

len(x)

df_fixed2.drop(index=x, inplace = True)
df_fixed2[(df_fixed2['denominator'].isnull()) & df_fixed2['dof_population'].isnull()]['geoname'].value_counts()

df = df_fixed2.copy()

"""For the final step for cleaning we will clean up the nulls in rate, ll_95ci, ul_95ci, etc"""

# populate the rate which is numerator/denominator (crime/population)

df['rate'] = df['numerator'] / df['denominator']

# we wil use the wilson meathod to deterimine the lower level and uper level confidence intervals.

from statsmodels.stats.proportion import proportion_confint

df['ll_95ci'] = proportion_confint(count=df['numerator'], nobs=df['denominator'], alpha=0.05,method='wilson')[0]

df['ul_95ci'] = proportion_confint(count=df['numerator'], nobs=df['denominator'], alpha=0.05,method='wilson')[1]

# next we will find the standard error

df['se'] = ((df['rate']*(1 - df['rate'])) / (df['denominator']))**0.5

# Now we will find the RSE

df['rse'] = (df['se'] / df['rate']) * 100

# Since we are looking at our data through the lens of crimes per 1000, we will multiply appropriate columns by 1000

df['rate'] = df['rate'] * 1000
df['ll_95ci'] = df['ll_95ci'] * 1000
df['ul_95ci'] = df['ul_95ci'] * 1000
df['se'] = df['se'] * 1000

# Finally we will drop some columns that dont have use for us
df.drop(columns=['race_eth_code', 'race_eth_name', 'strata_name','strata_name_code','ca_decile','ca_rr','dof_population'], inplace = True)

"""Finally, we are ready to start analyzing!

# Analysis

First lets look at state level statistics
"""

df_total_only = df[(df['strata_level_name_code'] == 5) & (df['geoname'] == 'California') & df['county_fips'].isnull()]
df_total_only.head(len(df['reportyear']))

import matplotlib.pyplot as plt

plt.plot(df_total_only['reportyear'],df_total_only['rate'])

plt.xlabel('Year')
plt.ylabel('Crime rate per 1000')
plt.title('California Crime Rate per 1000 from 2000-2013')
plt.show()

# View the year over year change of the crime rate for california from 2000 to 2013.

df_total_only['YoY_change'] = df_total_only['rate'].pct_change()

plt.plot(df_total_only['reportyear'],df_total_only['YoY_change'], label='violent crimes', color = 'green')

plt.xlabel('Year')
plt.ylabel('Rate in %')
plt.title('California Violent Crime Rate YoY Change in %')

plt.legend()
plt.show()

# overall we are seeing the rate of violent crime decrease in the state of California over time. Lets see how crime and population changed over time to see if that provides any insight

figure = plt.figure(figsize=(10,10))

plt.subplot(2,1,1)
plt.plot(df_total_only['reportyear'],df_total_only['numerator'], label='violent crimes', color = 'orange')

plt.xlabel('Year')
plt.ylabel('Number of Crimes')
plt.title('California Violent Crime from 2000-2013')
plt.legend()


plt.subplot(2,1,2)
plt.plot(df_total_only['reportyear'],df_total_only['denominator'], label='state population', color = 'black')

plt.xlabel('Year')
plt.ylabel('Population (in 10 million)')
plt.title('California State Population from 2000-2013')
plt.legend()
plt.show()

"""Not only did the number of violent crimes per year reduce, California's population per year increased as well. Effectively making the violent crime rate per 1000 lower. 
Perhaps there was legislation passed around the year of 2001 as that is when the decrease of violent crime starts to occur."""

# Now look at each crime type over the years. First make a df that exludes the totals
df_no_total = df[(df['strata_level_name_code'] != 5) & (df['geoname'] == 'California') & df['county_fips'].isnull()]
df_no_total.head()

# plot the rate of each crime in the state between 2000-2013

for name,group in df_no_total.groupby('strata_level_name'):
    plt.plot(group['reportyear'], group['rate'], label=name)
plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
plt.show()

"""Aggrevated assault and robery decreased over time while others remained relatively the same at a low rate. Interestingly enough robberies increased from 
2005-20006 and remained at a similar rate until 2008 where it began to drop again. The great recession was from 2007 - 2009 and our data shows crime generally fell 
during the great recession."""

# Lets view some of the regions with the highest crime rates

df_region = df[df['geotype']== 'RE']

df_region.head()

# first we need to see which measure of center will be the most useful for this data.

df_region.plot.hist(column='rate', by = 'region_name', figsize=(10,50))

plt.show()

'''because our data is not normally distributed and is heavily skewed right (on aggregate), our best measure of center would be the median. 
next we will find the median of the 'violent crime total' across all regions'''

median_region_rate = df_region[df_region['strata_level_name_code'] == 5].groupby('region_name')['rate'].median().sort_values(ascending=False)

# Create bar graph from above code


cat_med = median_region_rate.index
values_med = median_region_rate.values

figure = plt.figure(figsize=(10,10))

plt.subplot(3,1,1)

plt.bar(cat_med, values_med)
median_region_rate.plot(kind='bar')
plt.xlabel('Region')
plt.ylabel('Rate')
plt.title('Median Violent Crime Rate per Region')

plt.show()

"""Based on the figure above we can see which regions had the highest median crime rate between the years of 2000-2013 where San Joaquin Valley 
has the most and the San Louis Obispo has the lowest."""

'''which crimes had the highest median in  San Joaquin Valley, Southern California, and Shasta between 2000-2013?'''


top_three = 'San Joaquin Valley', 'Southern California', 'Shasta'
top_region = df_region[(df_region['region_name'].isin(top_three)) & (df_region['strata_level_name_code'] != 5)]

for name in top_three:
    top_region_crime = top_region[top_region['region_name'] == name].groupby('strata_level_name')['rate'].median().sort_values(ascending=False)

    cat_med = top_region_crime.index
    values_med = top_region_crime.values


    plt.bar(cat_med, values_med)
    top_region_crime.plot(kind='bar')
    plt.xticks(rotation=25)
    plt.xlabel('Crime Type')
    plt.ylabel('Rate')
    plt.title(f'Median Violent Crime Rate per Violent Crime in {name} from 2000-2013')

    plt.show()





#top_region_crime = top_region.groupby('strata_level_name')['rate'].median()

#top_region_crime.plot.bar()

# lets see how the rate of these crimes look over time

for region in top_three:
    x = top_region[(top_region['region_name'] == region) & (top_region['geotype']== 'RE')]
    for name,group in x.groupby('strata_level_name'):
        plt.plot(group['reportyear'], group['rate'], label = name)
    plt.xlabel('Year')
    plt.ylabel('Rate')
    plt.title(f'Median Violent Crime Rate Type Between 2000-2013 in {region}')
    plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    plt.show()

"""Interestingly we can see that aggrevated assault and robery follow a bit of a similar trend but robbery happens at a lower rate. 
We also see that in SJV those two crimes hardly decreased over time compared to the rest of the state. For Shasta aggrevated assult actually did the opposite 
and increased from 2000 to 2013. Shast also had sexual assualt rate compared to the other regions"""

# Now which counties are struggling with the highest rates of crime within these regions?

#first we will see how many counties there are in each region

top_three_county = df_region = df[(df['geotype']== 'CO') & (df['region_name'].isin(top_three))]

top_three_county.head()

x = top_three_county.groupby('county_name')['region_name'].unique().value_counts()
x

# Shasta only has one country so we will look at which counties in SJV and Southern California have the highest crime rates between 2000-2013

total_county = top_three_county[top_three_county['strata_level_name_code'] == 5]
total_county.head()

x = total_county.groupby('county_name')['rate'].median().sort_values(ascending=False)

cat_med = x.index
values_med = x.values

figure = plt.figure(figsize=(10,10))

plt.subplot(3,1,1)

plt.bar(cat_med, values_med)
x.plot(kind='bar')
plt.xlabel('County')
plt.ylabel('Rate')
plt.title('Median Violent Crime Rate per County')

plt.show()

"""Above are the top three counties with the highest crime rate per 1000 within the three regions with the highest crime rate. Most of these counties are within the SJV region."""

# look at the crime rates over time by crime type for each San Joaquin, Los Angeles, and Merced.

top_county = 'San Joaquin', 'Los Angeles', 'Merced'

for county in top_county:
    x = top_three_county[(top_three_county['county_name'] == county) & (top_three_county['geotype']== 'CO') & (top_three_county['strata_level_name_code'] != 5)]
    for name,group in x.groupby('strata_level_name'):
        plt.plot(group['reportyear'], group['rate'], label = name)
    plt.xlabel('Year')
    plt.ylabel('Rate')
    plt.title(f'Median Violent Crime Rate Type Between 2000-2013 in {county} County')
    plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    plt.show()

"""Los Angeles County followed a similar trend to state level data where aggrevated assult and robbery declined over time. Howver, Merced and San Joaquin County 
crime rate generally stayed the same when comparing year 2000 and year 2013 numbers. Some years were better than others but no concistent downward trending crime rate over time."""

# Lets look at which cities in these counties have the highest crime rates

top_city_county = df[(df['county_name'].isin(top_county)) & (df['geotype'] == 'PL')]

top_city_county.head()

# look at the median crime rate for all the cities in the counties with the highest crime rate. Use a histogram to view any outliers.

city_rate = top_city_county[top_city_county['strata_level_name_code'] == 5].groupby('geoname')['rate'].median()

#histogram
city_rate.plot.hist(column='rate',figsize=(10,6), bins=20)

# there are some outlier cities with crime rates that are absurdly high but our data shows that the majority of our data has crime rates well below 50.

# find top 3 cities that exlude outliers

x = top_city_county[(top_city_county['strata_level_name_code'] == 5) & (top_city_county['rate'] <50)].groupby('geoname')['rate'].median().sort_values(ascending=False).head(5)

plt.barh(x.index, x.values)
plt.xlabel('Crime rate per 1000')
plt.ylabel('City')

top_city_county.head()

# lets view the types of crimes

top_city = 'Compton city', 'Stockton city', 'Irwindale city'

for city in top_city:
    x = top_city_county[(top_city_county['geoname']==city) & (top_city_county['geotype']== 'PL') & (top_city_county['strata_level_name_code'] != 5)]
    for name,group in x.groupby('strata_level_name'):
        plt.plot(group['reportyear'], group['rate'], label = name)
    plt.xlabel('Year')
    plt.ylabel('Rate')
    plt.title(f'Median Violent Crime Rate Type Between 2000-2013 in {city}')
    plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
    plt.show()

"""although Compton's robbery and aggravated assults were high, they have generally decreased since 2000 but picking up in 2013. 
Stockton on the other hand did not show much change in crime rate as the it seems to be a similar start and end at similar rates. 
Irwindale is erratic, most likely due to a smaller population (in the thousands) but showed high crime rates during certain years. 
Interestingly, in Irwindale, robber, murder/NNM, and sexual assault happened at the same rate in 2011."""

# see where stockton and compton compare to the rest of our data

#check for outliers again

all_city= df[df['geotype'] == 'PL']

all_city_rate = all_city[all_city['strata_level_name_code'] == 5].groupby('geoname')['rate'].median()

#histogram
all_city_rate.plot.hist(column='rate',figsize=(10,6), bins=30)

# pretty similar to our other histogam, lets take a look at the cities with the highest crime rates, excluding outliers.


x = all_city[(all_city['strata_level_name_code'] == 5) & (all_city['rate'] <50)].groupby('geoname')['rate'].median().sort_values(ascending=False).head(5)

plt.barh(x.index, x.values)
plt.xlabel('Crime rate per 1000')
plt.ylabel('City')

"""Stockton and compton both make the top 5, but oakland comes in at #1. Emeryville is just noth of oakland which could explain the higher crime rate 
as there could be spill over. Sand City however has a high crime rate but their population is very low compared to the others on this list.

# Hypothesis Testing
"""

# creating data frame with southern california totals (sc)

sc = df[(df['region_name'] == 'Southern California') & (df['geotype'] == 'RE') & (df['strata_level_name_code'] == 5)]['rate']

# creating data frame with bay area totals (ba)
ba = df[(df['region_name'] == 'Bay Area') & (df['geotype'] == 'RE') & (df['strata_level_name_code'] == 5)]['rate']

# We will use the Mann-Whitney U non-parametric test since these observations are not normally distributed and independent.

# H0: crime rate per 1000 is the same between southern california and bay area (H0: sc = ba)
# Ha: crime rate per 1000 is the different between southern california and bay area (Ha: sc != ba)

import scipy.stats as stats

statistic, p_value = stats.mannwhitneyu(sc,ba)

if p_value < 0.05:
    print(f'The p-value is {p_value:.6}, meaning there is a statistical difference between groups. Thus, we can reject the null hypothesis.')
else:
    print(f'The p-value is {p_value:.6}, meaning there is not enough statistical evidence to prove a difference between groups. Thus, we fail to reject the null hypothesis.')

"""We do not have enough evidence to reject the null hypothesis of southern california's crime rate is the same as the bay area's crime rate. 
This means that the two regions actually do have similar crime rates and any observed differences between the two are not statistically significant."""

# what about comparing cities from our graph above? lets compare some of the biggest counties to each other. Oakland, Stockton, and Compton

#create a dataframe for each city

oak = df[(df['geoname'] == 'Oakland city') & (df['geotype'] == 'PL') & (df['strata_level_name_code'] == 5)]['rate']

stkt = df[(df['geoname'] == 'Stockton city') & (df['geotype'] == 'PL') & (df['strata_level_name_code'] == 5)]['rate']

comp = df[(df['geoname'] == 'Compton city') & (df['geotype'] == 'PL') & (df['strata_level_name_code'] == 5)]['rate']

# perform kruskal-wallis test to compare across cities

statistic, pvalue = stats.kruskal(oak,stkt,comp)

# Set up hypothesis statment:

# H0: The crime rate is the same between oakland, stockton and compton.
# Ha: The crime rate is different between oakland, stockton and compton.

if pvalue < 0.05:
    print(f'The p-value is {pvalue:.6}, meaning there is a statistical difference between groups. Thus, we can reject the null hypothesis.')
else:
    print(f'The p-value is {pvalue:.6}, meaning there is not enough statistical evidence to prove a difference between groups. Thus, we fail to reject the null hypothesis.')

"""After doing the Kruskal-Wallis test we found that there is a statistical significance between two of the group's medians, allowing us to reject the null hypothesis
in favor of the alternative hypothesis. The crime rate across the three counties are differtent. But which county is different? We will explore below."""

# we can determine which group is different from the others with a post hoc test.

# install posthoc package and import
!pip install scikit-posthocs
import scikit_posthocs as sp

# make list of each group's crime rate so it can be passed in post hoc function
data = [oak,stkt,comp]

# run post hoc analysis
sp.posthoc_dunn(data)

"""In the above table, group 1 is Oakland, group 2 is Stocton, and group 3 is Compton. We see that Oakland's adjusted p- vlaue is different than Stockton's (less than 0.05) 
and Compton's's is different than Stockton's. This means Oakland and Compton have statistically different crime rates when compared to Stockton's."""

print(f'Oakland: {oak.mean():.4f}')
print(f'Stockton: {stkt.mean():.4f}')
print(f'Compton: {comp.mean():.4f}')

"""The above medians help give more context. Oakland and Compton have similar crime rates while Stockton's is lower. 
Putting everything together we can see there is less crime in Stockton than the other two counties, less enough that it is 
statistically significant and not due to random chance."""

